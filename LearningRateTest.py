# -*- coding: utf-8 -*-
"""EE449HW1_5_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dG0VTE-Cf2ccTyShaEQTMX3XL-gfP7P0
"""

### Question 5 ###

# Import PyTorch library and optimization module
import torch
import torch.optim as optim

# Import torchvision library and numpy
import torchvision
import numpy as np
import torchvision.transforms as transforms

# Import train_test_split from scikit-learn
from sklearn.model_selection import train_test_split

# Check if GPU is available, else use CPU
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# CIFAR-10 images have a size of 32x32

# Define a series of image transformations to be applied to the CIFAR-10 dataset
transform = transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        torchvision.transforms.Grayscale()
])

# Load the CIFAR-10 dataset into a PyTorch data loader, applying the defined transformations to it
train_data = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)

# Split the training data into a training set and a validation set, with a split ratio of 90:10
train_set, val_data = train_test_split(train_data, test_size=0.1, random_state=42)

# Create a PyTorch data loader for the training set
train_generator = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True)

# Load the CIFAR-10 test dataset into a PyTorch data loader, applying the same transformations as for the training data
test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform)

# Create a PyTorch data loader for the test set
test_generator = torch.utils.data.DataLoader(test_data, batch_size=50, shuffle=False)

# Create a PyTorch data loader for the validation set
val_generator = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False)

class CNN4_R(torch.nn.Module):
    def __init__(self):
        super(CNN4_R, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # Conv. 5x5x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu3 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 5x5x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu4 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding = 0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*4*4, out_features=10)

    def forward(self, x):
        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30])
        x = self.relu1(x)
        x = self.conv2(x)
        # torch.Size([50, 8, 28, 28])
        x = self.relu2(x)
        x = self.conv3(x)
        # torch.Size([50, 16, 24, 24])
        x = self.relu3(x)
        x = self.maxpool1(x)
        # torch.Size([50, 16, 12, 12])
        x = self.conv4(x)
        x = self.relu4(x)
        # torch.Size([50, 16, 8, 8])
        x = self.maxpool2(x)
        # torch.Size([50, 16, 4, 4])
        x = x.view(50 ,16*4*4)
        # torch.Size([50, 256])
        x = self.prediction_layer(x)
        return x

# Model name
model_name = 'CNN4_R'

# Batch size is 50
batch_size = 50
# Number of epochs is 20
num_epochs = 20

lr_list = [0.1, 0.01, 0.001]
train_loss_complete = []
val_acc_complete = []

# Train the model 3 times for different learning rates (0.1, 0.01, 0.001)
for x in range(len(lr_list)):

  model = CNN4_R() 
  model.to(device)
  # Determine the loss function
  criterion = torch.nn.CrossEntropyLoss()
  # Using SGD optimizer instead of the Adam optimizer
  optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0)

  # Initializing lists
  train_loss = []
  val_acc = []

  for epoch in range(num_epochs):

      for i, (images, labels) in enumerate(train_generator):

          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()

          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)

          images = images.to(device)
          labels = labels.to(device)
          
          # Forward pass and compute loss
          outputs = model(images)
          loss = criterion(outputs, labels)
          
          # Backward and optimize
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
          

          # Record training accuracy every 10 steps
          if (i+1) % 10 == 0:

              # Record training loss
              train_loss.append(loss.item())

              with torch.no_grad():
                  total = 0
                  correct = 0

                  #outputs = model(images)
                  _, predicted = torch.max(outputs.data, 1)
                  total += labels.size(0)
                  correct += (predicted == labels).sum().item()
                  train_accuracy = 100 * correct / total

                  # Record validation accuracy every 10 steps
                  total = 0
                  correct = 0
                  for images, labels in val_generator:
                      images = images.to(device)
                      labels = labels.to(device)
                      outputs = model(images)
                      _, predicted = torch.max(outputs.data, 1)
                      total += labels.size(0)
                      correct += (predicted == labels).sum().item()
                  val_accuracy = 100 * correct / total
                  val_acc.append(val_accuracy)

      # To find out, at which step test accuracy converges to some value
      print('Val. Acc.: ', val_accuracy) 
      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
            .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))


  print(len(train_loss))
  # Test the model
  model.eval()
  with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in test_generator:

          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()

          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)
          
          images = images.to(device)
          labels = labels.to(device)
          
          # Compute predictions
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_acc = 100 * correct / total
      print('Accuracy of the model on the test images: {} %'.format(test_acc))
  
  train_loss_complete.append(train_loss)
  val_acc_complete.append(val_acc)

import pickle

# create the dictionary object with the required key-value pairs
result_dict = {
    
    'name': model_name,
    'loss_curve_1': train_loss_complete[0],
    'loss_curve_01': train_loss_complete[1],
    'loss_curve_001': train_loss_complete[2],
    'val_acc_curve_1': val_acc_complete[0],
    'val_acc_curve_01': val_acc_complete[1],
    'val_acc_curve_001': val_acc_complete[2]
    
}

# save the dictionary object to a file
filename = 'part5_'+str(model_name)+'.pkl'
with open(filename, 'wb') as file:
    pickle.dump(result_dict, file)

# load the dictionary object from the file
with open(filename, 'rb') as file:
    loaded_dict = pickle.load(file)

# print the loaded dictionary object
print(loaded_dict)

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
import os
import torch
from torchvision.utils import make_grid

# utility function to create performance plots for part 5
def part5Plots(result, save_dir='', filename='', show_plot=True):
  
    if isinstance(result, (list, tuple)):
        result = result[0]

    color_list = ['#0000ff', '#ff0000', '#d2691e', '#ff00ff', '#00ff00', '#000000', '#373788']
    style_list = ['-', '--']

    num_curves = 3

    plot_args = [{'c': color_list[k],
                        'linestyle': style_list[0],
                        'linewidth': 2} for k in range(num_curves)]


    key_suffixes = ['1', '01', '001']


    font_size = 18

    fig, axes = plt.subplots(1, 2, figsize=(16, 12))

    fig.suptitle('training of <%s> with different learning rates'%result['name'],
                 fontsize=font_size, y=0.025)

    # training loss and validation accuracy
    axes[0].set_title('training_losses', loc='left', fontsize=font_size)
    axes[1].set_title('validation_accuracies', loc='right', fontsize=font_size)
    for key_suffix, plot_args in zip(key_suffixes, plot_args):

        loss_curve = result['loss_curve_' + key_suffix]
        acc_curve = result['val_acc_curve_' + key_suffix]
        label = 'lr=0.%s'%key_suffix

        axes[0].plot(np.arange(1, len(loss_curve) + 1),
                     loss_curve, label=label, **plot_args)
        axes[0].set_xlabel(xlabel='step', fontsize=font_size)
        axes[0].set_ylabel(ylabel='loss', fontsize=font_size)
        axes[0].tick_params(labelsize=12)

        axes[1].plot(np.arange(1, len(acc_curve) + 1),
                     acc_curve, label=label, **plot_args)
        axes[1].set_xlabel(xlabel='step', fontsize=font_size)
        axes[1].set_ylabel(ylabel='accuracy', fontsize=font_size)
        axes[1].tick_params(labelsize=12)


    # global legend
    lines = axes[0].get_lines()
    fig.legend(labels=[line._label for line in lines],
               ncol=3, loc="upper center", fontsize=font_size,
               handles=lines)

    if show_plot:
        plt.show()

    fig.savefig(os.path.join(save_dir, filename + '.png'))

part5Plots(loaded_dict, save_dir=r'data', filename='part5Plots')

# Model name
model_name = 'CNN4_R'

# Batch size is 50
batch_size = 50
# Number of epochs is 20
num_epochs = 30

train_loss_complete = []
val_acc_complete = []

# Now, learning rate is changed to 0.01 after 8th epoch.
# Learning rate is decreased to conitnue the process of increasing the validation accuracy
# Create model, loss function, and optimizer
model = CNN4_R() 
model.to(device)
# Determine the loss function
criterion = torch.nn.CrossEntropyLoss()
# Using SGD optimizer instead of the Adam optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0)

# Initializing lists
train_loss = []
val_acc = []

for epoch in range(num_epochs):

  # At epoch = 10, lr = 0.1 increases the validation accuracy no more
  # lr is changed to 0.01
  if(epoch == 10):
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum = 0)


  for i, (images, labels) in enumerate(train_generator):

      # Convert data to PyTorch tensors
      images = images.numpy()
      labels = labels.numpy()

      images = torch.from_numpy(images)
      labels = torch.from_numpy(labels)

      images = images.to(device)
      labels = labels.to(device)
      
      # Forward pass and compute loss
      outputs = model(images)
      loss = criterion(outputs, labels)
      
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      

      # Record training accuracy every 10 steps
      if (i+1) % 10 == 0:

          # Record training loss
          train_loss.append(loss.item())

          with torch.no_grad():
              total = 0
              correct = 0

              #outputs = model(images)
              _, predicted = torch.max(outputs.data, 1)
              total += labels.size(0)
              correct += (predicted == labels).sum().item()
              train_accuracy = 100 * correct / total

              # Record validation accuracy every 10 steps
              total = 0
              correct = 0
              for images, labels in val_generator:
                  images = images.to(device)
                  labels = labels.to(device)
                  outputs = model(images)
                  _, predicted = torch.max(outputs.data, 1)
                  total += labels.size(0)
                  correct += (predicted == labels).sum().item()
              val_accuracy = 100 * correct / total
              val_acc.append(val_accuracy)

  # To find out, at which step test accuracy converges to some value
  print('Val. Acc.: ', val_accuracy) 
  print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
        .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))


  # Test the model
  model.eval()
  with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in test_generator:

          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()

          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)

          images = images.to(device)
          labels = labels.to(device)
          
  
  train_loss_complete.append(train_loss)
  val_acc_complete.append(val_acc)

import pickle

# create the dictionary object with the required key-value pairs
result_dict = {
    
    'name': model_name,
    'loss_curve_1': train_loss_complete[0],
    'loss_curve_01': train_loss_complete[1],
    'loss_curve_001': train_loss_complete[2],
    'val_acc_curve_1': val_acc_complete[0],
    'val_acc_curve_01': val_acc_complete[1],
    'val_acc_curve_001': val_acc_complete[2]
    
}

# save the dictionary object to a file
filename = 'part5_'+str(model_name)+'.pkl'
with open(filename, 'wb') as file:
    pickle.dump(result_dict, file)

# load the dictionary object from the file
with open(filename, 'rb') as file:
    loaded_dict = pickle.load(file)

# print the loaded dictionary object
print(loaded_dict)

part5Plots(loaded_dict, save_dir=r'data', filename='part5Plots')

# Model name
model_name = 'CNN4_R'

# Batch size is 50
batch_size = 50
# Number of epochs is 20
num_epochs = 30

train_loss_complete = []
val_acc_complete = []

# Now, learning rate is changed to 0.01 after 8th epoch.
# Learning rate is decreased to conitnue the process of increasing the validation accuracy
# Create model, loss function, and optimizer
model = CNN4_R() 
model.to(device)
# Determine the loss function
criterion = torch.nn.CrossEntropyLoss()
# Using SGD optimizer instead of the Adam optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0)

# Initializing lists
train_loss = []
val_acc = []

for epoch in range(num_epochs):

  # At epoch = 10, lr = 0.1 increases the validation accuracy no more
  # lr is changed to 0.01
  if(epoch == 10):
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum = 0)
  # At epoch = 15, lr = 0.01 increases the validation accuracy no more
  # lr is changed to 0.001
  elif(epoch ==15):
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0)

  for i, (images, labels) in enumerate(train_generator):

      # Convert data to PyTorch tensors
      images = images.numpy()
      labels = labels.numpy()

      images = torch.from_numpy(images)
      labels = torch.from_numpy(labels)

      images = images.to(device)
      labels = labels.to(device)
      
      # Forward pass and compute loss
      outputs = model(images)
      loss = criterion(outputs, labels)
      
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      

      # Record training accuracy every 10 steps
      if (i+1) % 10 == 0:

          # Record training loss
          train_loss.append(loss.item())

          with torch.no_grad():
              total = 0
              correct = 0

              #outputs = model(images)
              _, predicted = torch.max(outputs.data, 1)
              total += labels.size(0)
              correct += (predicted == labels).sum().item()
              train_accuracy = 100 * correct / total

              # Record validation accuracy every 10 steps
              total = 0
              correct = 0
              for images, labels in val_generator:
                  images = images.to(device)
                  labels = labels.to(device)
                  outputs = model(images)
                  _, predicted = torch.max(outputs.data, 1)
                  total += labels.size(0)
                  correct += (predicted == labels).sum().item()
              val_accuracy = 100 * correct / total
              val_acc.append(val_accuracy)

  # To find out, at which step test accuracy converges to some value
  print('Val. Acc.: ', val_accuracy) 
  print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
        .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))


  # Test the model
  model.eval()
  with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in test_generator:

          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()

          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)

          images = images.to(device)
          labels = labels.to(device)
          
  
  train_loss_complete.append(train_loss)
  val_acc_complete.append(val_acc)

import pickle

# create the dictionary object with the required key-value pairs
result_dict = {
    
    'name': model_name,
    'loss_curve_1': train_loss_complete[0],
    'loss_curve_01': train_loss_complete[1],
    'loss_curve_001': train_loss_complete[2],
    'val_acc_curve_1': val_acc_complete[0],
    'val_acc_curve_01': val_acc_complete[1],
    'val_acc_curve_001': val_acc_complete[2]
    
}

# save the dictionary object to a file
filename = 'part5_'+str(model_name)+'.pkl'
with open(filename, 'wb') as file:
    pickle.dump(result_dict, file)

# load the dictionary object from the file
with open(filename, 'rb') as file:
    loaded_dict = pickle.load(file)

# print the loaded dictionary object
print(loaded_dict)

part5Plots(loaded_dict, save_dir=r'data', filename='part5Plots')

### End of Question 5 ###