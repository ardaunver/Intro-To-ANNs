# -*- coding: utf-8 -*-
"""EE449HW1_4_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h-vxIRu0CpRXd1AhNI3zEF0XBLt5gpgg
"""

### Question 4 ###

# Import PyTorch library and optimization module
import torch
import torch.optim as optim

# Import torchvision library and numpy
import torchvision
import numpy as np
import torchvision.transforms as transforms

# Import train_test_split from scikit-learn
from sklearn.model_selection import train_test_split

# Check if GPU is available, else use CPU
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# CIFAR-10 images have a size of 32x32

# Define a series of image transformations to be applied to the CIFAR-10 dataset
transform = transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        torchvision.transforms.Grayscale()
])

# Load the CIFAR-10 dataset into a PyTorch data loader, applying the defined transformations to it
train_data = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)

# Split the training data into a training set and a validation set, with a split ratio of 90:10
train_set, val_data = train_test_split(train_data, test_size=0.1, random_state=42)

# Create a PyTorch data loader for the training set
train_generator = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True)

# Load the CIFAR-10 test dataset into a PyTorch data loader, applying the same transformations as for the training data
test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform)

# Create a PyTorch data loader for the test set
test_generator = torch.utils.data.DataLoader(test_data, batch_size=50, shuffle=False)

# Create a PyTorch data loader for the validation set
val_generator = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False)

# Define the MLP models
class MLP1_R(torch.nn.Module):
    def __init__(self):
        super(MLP1_R, self).__init__()
        # Defining the Layers
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 10,bias = True)

    def forward(self, x):
        # Connecting the layers
        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        # x is now [50,1024] since batch size is 50 
        # and we linearize the input 32*32 (input size) to 1024*1
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

class MLP1_S(torch.nn.Module):
    def __init__(self):
        super(MLP1_S, self).__init__()
        # Defining the Layers
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 10,bias = True)

    def forward(self, x):
        # Connecting the layers
        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        # x is now [50,1024] since batch size is 50 
        # and we linearize the input 32*32 (input size) to 1024*1
        x = self.fc1(x)
        x = torch.sigmoid(x)
        x = self.fc2(x)
        return x

class MLP2_R(torch.nn.Module):
    def __init__(self):
        super(MLP2_R, self).__init__()
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 64, bias = False)
        self.prediction_layer = torch.nn.Linear(64, 10,bias = False)

    def forward(self, x):

        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.prediction_layer(x)
        return x

class MLP2_S(torch.nn.Module):
    def __init__(self):
        super(MLP2_S, self).__init__()
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 64, bias = False)
        self.prediction_layer = torch.nn.Linear(64, 10,bias = False)

    def forward(self, x):

        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        x = self.fc1(x)
        x = torch.sigmoid(x)
        x = self.fc2(x)
        x = self.prediction_layer(x)
        return x


# Define the CNN models
# To track the dimensions of the outputs of the layers,
# print(x.shape) is used between the lines
class CNN3_R(torch.nn.Module):
    def __init__(self):
        super(CNN3_R, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 5x5x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 7x7x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (7,7), stride = 1, padding = 'valid')
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*3*3, out_features=10)

        # (height,width) - kernel_size + 1 = output dimensions !!! 

    def forward(self, x):
        # torch.Size([50, 1, 32, 32]) , 50 32x32 images in one batch
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30]) , after CONV.3x3x16
        x = self.relu1(x)
        # torch.Size([50, 16, 30, 30]), RelU does NOT effect the dimensions
        x = self.conv2(x)
        # torch.Size([50, 8, 26, 26]) , after CONV.5x5x8
        x = self.relu2(x)
        x = self.maxpool(x)
        # torch.Size([50, 8, 13, 13]) , after MaxPool2d (height and width are two times smaller)
        x = self.conv3(x)
        # torch.Size([50, 16, 7, 7]) , after CONV.7x7x16
        x = self.maxpool(x)
        # torch.Size([50, 16, 3, 3]) , after MaxPool2d
        x = x.view(50 , 16*3*3)
        # torch.Size([50, 144]) , after x.view
        x = self.prediction_layer(x)

        return x

class CNN3_S(torch.nn.Module):
    def __init__(self):
        super(CNN3_S, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid1 = torch.nn.Sigmoid()
        # Conv. 5x5x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(5,5), stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid2 = torch.nn.Sigmoid()
        # MaxPool 2x2
        self.maxpool = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 7x7x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (7,7), stride = 1, padding = 'valid')
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*3*3, out_features=10)

        # (height,width) - kernel_size + 1 = output dimensions !!! 

    def forward(self, x):
        # torch.Size([50, 1, 32, 32]) , 50 32x32 images in one batch
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30]) , after CONV.3x3x16
        x = self.sigmoid1(x)
        # torch.Size([50, 16, 30, 30]), RelU does NOT effect the dimensions
        x = self.conv2(x)
        # torch.Size([50, 8, 26, 26]) , after CONV.5x5x8
        x = self.sigmoid2(x)
        x = self.maxpool(x)
        # torch.Size([50, 8, 13, 13]) , after MaxPool2d (height and width are two times smaller)
        x = self.conv3(x)
        # torch.Size([50, 16, 7, 7]) , after CONV.7x7x16
        x = self.maxpool(x)
        # torch.Size([50, 16, 3, 3]) , after MaxPool2d
        x = x.view(50 , 16*3*3)
        # torch.Size([50, 144]) , after x.view
        x = self.prediction_layer(x)

        return x

class CNN4_R(torch.nn.Module):
    def __init__(self):
        super(CNN4_R, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # Conv. 5x5x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu3 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 5x5x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu4 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding = 0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*4*4, out_features=10)

    def forward(self, x):
        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30])
        x = self.relu1(x)
        x = self.conv2(x)
        # torch.Size([50, 8, 28, 28])
        x = self.relu2(x)
        x = self.conv3(x)
        # torch.Size([50, 16, 24, 24])
        x = self.relu3(x)
        x = self.maxpool1(x)
        # torch.Size([50, 16, 12, 12])
        x = self.conv4(x)
        x = self.relu4(x)
        # torch.Size([50, 16, 8, 8])
        x = self.maxpool2(x)
        # torch.Size([50, 16, 4, 4])
        x = x.view(50 ,16*4*4)
        # torch.Size([50, 256])
        x = self.prediction_layer(x)
        return x

class CNN4_S(torch.nn.Module):
    def __init__(self):
        super(CNN4_S, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid1 = torch.nn.Sigmoid()
        # Conv. 3x3x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid2 = torch.nn.Sigmoid()
        # Conv. 5x5x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(5,5), stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid3 = torch.nn.Sigmoid()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 5x5x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (5,5), stride = 1, padding = 'valid')
        # Sigmoid layer
        self.sigmoid4 = torch.nn.Sigmoid()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding = 0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*4*4, out_features=10)

    def forward(self, x):
        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30])
        x = self.sigmoid1(x)
        x = self.conv2(x)
        # torch.Size([50, 8, 28, 28])
        x = self.sigmoid2(x)
        x = self.conv3(x)
        # torch.Size([50, 16, 24, 24])
        x = self.sigmoid3(x)
        x = self.maxpool1(x)
        # torch.Size([50, 16, 12, 12])
        x = self.conv4(x)
        x = self.sigmoid4(x)
        # torch.Size([50, 16, 8, 8])
        x = self.maxpool2(x)
        # torch.Size([50, 16, 4, 4])
        x = x.view(50 ,16*4*4)
        # torch.Size([50, 256])
        x = self.prediction_layer(x)
        return x


class CNN5_R(torch.nn.Module):
    def __init__(self):
        super(CNN5_R, self).__init__()
        # Conv. 3x3x8 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 3x3x16 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # ReLU layer
        self.relu3 = torch.nn.ReLU()
        # Conv. 3x3x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu4 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 3x3x16 Layer
        self.conv5 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu5 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv6 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # ReLU layer
        self.relu6 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 8*4*4, out_features=10)


    def forward(self, x):

        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # Size = [50, 8, 30, 30] (guessed)
        x = self.relu1(x)
        x = self.conv2(x)
        # Size = [50, 16, 28, 28] (guessed)
        x = self.relu2(x)
        x = self.conv3(x)
        # Size = [50, 8, 26, 26] (guessed)
        x = self.relu3(x)
        x = self.conv4(x)
        # Size = [50, 16, 24, 24] (guessed)
        x = self.relu4(x)
        x = self.maxpool1(x)
        # Size = [50, 16, 12, 12] (guessed) 
        # MaxPool does NOT efect number of output channels
        x = self.conv5(x)
        # Size = [50, 16, 10, 10] (guessed)
        x = self.relu5(x)
        x = self.conv6(x)
        # Size = [50, 8, 8, 8] (guessed)
        x = self.relu6(x)
        x = self.maxpool2(x)
        # Size = [50, 8, 4, 4] (guessed)
        # torch.Size([50, 8, 4, 4]) (the guess was correct)
        x = x.view(50 ,8*4*4)
        
        x = self.prediction_layer(x)

        
        return x

      
class CNN5_S(torch.nn.Module):
    def __init__(self):
        super(CNN5_S, self).__init__()
        # Conv. 3x3x8 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size= (3,3) , stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid1 = torch.nn.Sigmoid()
        # Conv. 3x3x16 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid2 = torch.nn.Sigmoid()
        # Conv. 3x3x8 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid3 = torch.nn.Sigmoid()
        # Conv. 3x3x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid4 = torch.nn.Sigmoid()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 3x3x16 Layer
        self.conv5 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid5 = torch.nn.Sigmoid()
        # Conv. 3x3x8 Layer
        self.conv6 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # Sigmoid layer
        self.sigmoid6 = torch.nn.Sigmoid()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 8*4*4, out_features=10)


    def forward(self, x):

        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # Size = [50, 8, 30, 30] (guessed)
        x = self.sigmoid1(x)
        x = self.conv2(x)
        # Size = [50, 16, 28, 28] (guessed)
        x = self.sigmoid2(x)
        x = self.conv3(x)
        # Size = [50, 8, 26, 26] (guessed)
        x = self.sigmoid3(x)
        x = self.conv4(x)
        # Size = [50, 16, 24, 24] (guessed)
        x = self.sigmoid4(x)
        x = self.maxpool1(x)
        # Size = [50, 16, 12, 12] (guessed) 
        # MaxPool does NOT efect number of output channels
        x = self.conv5(x)
        # Size = [50, 16, 10, 10] (guessed)
        x = self.sigmoid5(x)
        x = self.conv6(x)
        # Size = [50, 8, 8, 8] (guessed)
        x = self.sigmoid6(x)
        x = self.maxpool2(x)
        # Size = [50, 8, 4, 4] (guessed)
        # torch.Size([50, 8, 4, 4]) (the guess was correct)
        x = x.view(50 ,8*4*4)
        
        x = self.prediction_layer(x)

        
        return x

# Change the model and the 2 models to be trained (for RelU and Sigmoid)

# Model name
model_name = 'CNN5'
# Number of tests
num_test = 10

# Batch size is 50
batch_size = 50
# Number of epochs is 15
num_epochs = 15


# Create model, loss function, and optimizer for the model with ReLU activation
model = CNN5_R() 
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Initialize the lists
relu_loss_curve = []
relu_grad_curve = []

# Shuffle train generator at each epoch
for epoch in range(num_epochs): 

    for i, (images, labels) in enumerate(train_generator):
        # Convert data to PyTorch tensors
        images = images.numpy()
        labels = labels.numpy()
        images = torch.from_numpy(images)
        labels = torch.from_numpy(labels)
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass and compute loss
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        

        # Record training accuracy every 10 steps
        if (i+1) % 10 == 0:

            # Use model.fc1 for MLPs, model.conv1
            first_layer_weights = model.conv1.weight.data.cpu().numpy()
            grad_mag = np.linalg.norm(model.conv1.weight.grad.data.cpu().numpy())
            relu_grad_curve.append(grad_mag)

            # Record training loss
            relu_loss_curve.append(loss.item())


        # Print training loss every 100 steps
        if (i+1) % 100 == 0:
             print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Grad Mag: {:.4f}'
                  .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item(), grad_mag))


# Test the model
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_generator:
        # Convert data to PyTorch tensors
        images = images.numpy()
        labels = labels.numpy()
        images = torch.from_numpy(images)
        labels = torch.from_numpy(labels)
        images = images.to(device)
        labels = labels.to(device)
        
        # Compute predictions
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    test_acc = 100 * correct / total
    print('Accuracy of the model on the test images: {} %'.format(test_acc))

# Model with Sigmoid Function as Activation Function 
# Create model, loss function, and optimizer for the model with ReLU activation
model = CNN5_S() 
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

sigmoid_loss_curve = []
sigmoid_grad_curve = []

# Shuffle train generator at each epoch
for epoch in range(num_epochs): 

    for i, (images, labels) in enumerate(train_generator):
        # Convert data to PyTorch tensors
        images = images.numpy()
        labels = labels.numpy()
        images = torch.from_numpy(images)
        labels = torch.from_numpy(labels)
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass and compute loss
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        

        # Record training accuracy every 10 steps
        if (i+1) % 10 == 0:

            # Use model.fc1 for MLPs, model.conv1
            first_layer_weights = model.conv1.weight.data.cpu().numpy()
            grad_mag = np.linalg.norm(model.conv1.weight.grad.data.cpu().numpy())
            sigmoid_grad_curve.append(grad_mag)

            # Record training loss
            sigmoid_loss_curve.append(loss.item())


        # Print training loss every 100 steps
        if (i+1) % 100 == 0:
             print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Grad Mag: {:.4f}'
                  .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item(), grad_mag))


# Test the model
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_generator:
        # Convert data to PyTorch tensors
        images = images.numpy()
        labels = labels.numpy()
        images = torch.from_numpy(images)
        labels = torch.from_numpy(labels)
        images = images.to(device)
        labels = labels.to(device)
        
        # Compute predictions
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        test_acc = 100 * correct / total
    print('Accuracy of the model on the test images: {} %'.format(test_acc))

import pickle

# create the dictionary object with the required key-value pairs
result_dict = {
    'name': model_name,
    'relu_loss_curve': relu_loss_curve,
    'sigmoid_loss_curve': sigmoid_loss_curve,
    'relu_grad_curve': relu_grad_curve,
    'sigmoid_grad_curve': sigmoid_grad_curve,
}

# save the dictionary object to a file
filename = 'part4_'+str(model_name)+'.pkl'
with open(filename, 'wb') as file:
    pickle.dump(result_dict, file)

# load the dictionary object from the file
with open(filename, 'rb') as file:
    loaded_dict = pickle.load(file)

# print the loaded dictionary object
print(loaded_dict)

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
import os
import torch
from torchvision.utils import make_grid

# utility function to create performance plots for part 4
def part4Plots(results, save_dir='', filename='', show_plot=True):
    """plots multiple performance curves from multiple training results and
    saves the resultant plot as a png image

    Arguments:
    ----------

    results: list of dictionary objects, each corresponds to
    the result of a training and should have the following key-value
    items:

        'name': string, indicating the user-defined name of the training

        'relu_loss_curve': list of floats, indicating the loss at each step

        'sigmoid_loss_curve': list of floats, indicating the loss at each step

        'relu_grad_curve': list of floats, indicating the gradient magnitude at each step

        'sigmoid_grad_curve': list of floats, indicating the gradient magnitude at each step

    save_dir: string, indicating the path to directory where the plot image is to be saved

    filename: string, indicating the name of the image file. Note that .png will be automatically
    appended to the filename.

    show_plot: bool, whether the figure is to be shown

    Example:
    --------

    visualizing the results of the training

    # assume the '*_value's are known

    >>> result_x = {'name': name_value, 'relu_loss_curve': relu_loss_curve_value,
                    'sigmoid_loss_curve': sigmoid_loss_curve_value,
                     'relu_grad_curve': relu_grad_curve_value}

    >>> results = [result_1, ..., result_x, ..., result_N]

    >>> part3Plots(results, save_dir=r'some\location\to\save', filename='part3Plots')

    """

    color_list = ['#0000ff', '#ff0000', '#d2691e', '#ff00ff', '#00ff00', '#000000', '#373788']
    style_list = ['-', '--']

    num_results = len(results)

    relu_curve_args = [{'c': color_list[k],
                        'linestyle': style_list[0],
                        'linewidth': 2} for k in range(num_results)]

    sigmoid_curve_args = [{'c': color_list[k],
                        'linestyle': style_list[1],
                        'linewidth': 2} for k in range(num_results)]



    font_size = 18

    fig, axes = plt.subplots(1, 2, figsize=(16, 12))

    legend_elements = [Line2D([0], [0], color='k', linestyle=style_list[0], lw=2, label='ReLU'),
                       Line2D([0], [0], color='k', linestyle=style_list[1], lw=2, label='Sigmoid')]

    # training loss
    ax = axes[0]
    ax.set_title('training_losses', loc='left', fontsize=font_size)
    for result, relu_args, sigmoid_args in zip(results, relu_curve_args, sigmoid_curve_args):
        ax.plot(np.arange(1, len(result['relu_loss_curve']) + 1),
                result['relu_loss_curve'], label=result['name'], **relu_args)
        ax.plot(np.arange(1, len(result['sigmoid_loss_curve']) + 1),
                result['sigmoid_loss_curve'], label=result['name'], **sigmoid_args)
        ax.set_xlabel(xlabel='step', fontsize=font_size)
        ax.set_ylabel(ylabel='loss', fontsize=font_size)
        ax.tick_params(labelsize=12)
        ax.legend(loc='upper left', fontsize=12, handles=legend_elements)

    # gradient magnitude
    ax = axes[1]
    ax.set_title('gradient_magnitudes', loc='right', fontsize=font_size)
    for result, relu_args, sigmoid_args in zip(results, relu_curve_args, sigmoid_curve_args):
        ax.plot(np.arange(1, len(result['relu_grad_curve']) + 1),
                result['relu_grad_curve'], label=result['name'], **relu_args)
        ax.plot(np.arange(1, len(result['sigmoid_grad_curve']) + 1),
                result['sigmoid_grad_curve'], label=result['name'], **sigmoid_args)
        ax.set_xlabel(xlabel='step', fontsize=font_size)
        ax.set_ylabel(ylabel='|grad_loss|', fontsize=font_size)
        ax.tick_params(labelsize=12)
        ax.legend(loc='upper right', fontsize=12, handles=legend_elements)

    # global legend
    lines = ax.get_lines()[::2]
    fig.legend(labels=[line._label for line in lines],
               ncol=3, loc="upper center", fontsize=font_size,
               handles=lines)

    if show_plot:
        plt.show()

    fig.savefig(os.path.join(save_dir, filename + '.png'))

# load the dictionary object from the file
with open('part4_MLP1.pkl', 'rb') as file:
    loaded_dict1 = pickle.load(file)

# load the dictionary object from the file
with open('part4_MLP2.pkl', 'rb') as file:
    loaded_dict2 = pickle.load(file)

# load the dictionary object from the file
with open('part4_CNN3.pkl', 'rb') as file:
    loaded_dict3 = pickle.load(file)

# load the dictionary object from the file
with open('part4_CNN4.pkl', 'rb') as file:
    loaded_dict4 = pickle.load(file)

# load the dictionary object from the file
with open('part4_CNN5.pkl', 'rb') as file:
    loaded_dict5 = pickle.load(file)

results = [loaded_dict1, loaded_dict2, loaded_dict3, loaded_dict4, loaded_dict5]

part4Plots(results, save_dir=r'data', filename='part4Plots')

### End of Question 4 ###