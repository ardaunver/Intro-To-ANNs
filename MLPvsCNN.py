# -*- coding: utf-8 -*-
"""EE449HW1_3_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USCVRJm_FvF-GphSylCf-radcP_VukXY
"""

### Question 3 ###


# Import PyTorch library and optimization module
import torch
import torch.optim as optim

# Import torchvision library and numpy
import torchvision
import numpy as np
import torchvision.transforms as transforms

# Import train_test_split from scikit-learn
from sklearn.model_selection import train_test_split

# Check if GPU is available, else use CPU
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

# CIFAR-10 images have a size of 32x32

# Define a series of image transformations to be applied to the CIFAR-10 dataset
transform = transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        torchvision.transforms.Grayscale()
])

# Load the CIFAR-10 dataset into a PyTorch data loader, applying the defined transformations to it
train_data = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)

# Split the training data into a training set and a validation set, with a split ratio of 90:10
train_set, val_data = train_test_split(train_data, test_size=0.1, random_state=42)

# Create a PyTorch data loader for the training set
train_generator = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True)

# Load the CIFAR-10 test dataset into a PyTorch data loader, applying the same transformations as for the training data
test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform)

# Create a PyTorch data loader for the test set
test_generator = torch.utils.data.DataLoader(test_data, batch_size=50, shuffle=False)

# Create a PyTorch data loader for the validation set
val_generator = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False)

# Define the MLP models
class MLP1(torch.nn.Module):
    def __init__(self):
        super(MLP1, self).__init__()
        # Defining the Layers
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 10,bias = True)

    def forward(self, x):
        # Connecting the layers
        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        # x is now [50,1024] since batch size is 50 
        # and we linearize the input 32*32 (input size) to 1024*1
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

class MLP2(torch.nn.Module):
    def __init__(self):
        super(MLP2, self).__init__()
        self.fc1 = torch.nn.Linear(1024, 32,bias = True)
        self.fc2 = torch.nn.Linear(32, 64, bias = False)
        self.prediction_layer = torch.nn.Linear(64, 10,bias = False)

    def forward(self, x):

        x = x.view(-1, 1024)  # Flatten input tensor 
        #print('x shape: ', x.shape)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.prediction_layer(x)
        return x


# Define the CNN models
# To track the dimensions of the outputs of the layers,
# print(x.shape) is used between the lines
class CNN3(torch.nn.Module):
    def __init__(self):
        super(CNN3, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 5x5x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 7x7x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (7,7), stride = 1, padding = 'valid')
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*3*3, out_features=10)

        # (height,width) - kernel_size + 1 = output dimensions !!! 

    def forward(self, x):
        # torch.Size([50, 1, 32, 32]) , 50 32x32 images in one batch
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30]) , after CONV.3x3x16
        x = self.relu1(x)
        # torch.Size([50, 16, 30, 30]), RelU does NOT effect the dimensions
        x = self.conv2(x)
        # torch.Size([50, 8, 26, 26]) , after CONV.5x5x8
        x = self.relu2(x)
        x = self.maxpool(x)
        # torch.Size([50, 8, 13, 13]) , after MaxPool2d (height and width are two times smaller)
        x = self.conv3(x)
        # torch.Size([50, 16, 7, 7]) , after CONV.7x7x16
        x = self.maxpool(x)
        # torch.Size([50, 16, 3, 3]) , after MaxPool2d
        x = x.view(50 , 16*3*3)
        # torch.Size([50, 144]) , after x.view
        x = self.prediction_layer(x)

        return x

class CNN4(torch.nn.Module):
    def __init__(self):
        super(CNN4, self).__init__()
        # Conv. 3x3x16 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size= (3,3) , stride = 1, padding = 'valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # Conv. 5x5x16 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu3 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 5x5x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (5,5), stride = 1, padding = 'valid')
        # ReLU layer
        self.relu4 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding = 0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 16*4*4, out_features=10)

    def forward(self, x):
        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # torch.Size([50, 16, 30, 30])
        x = self.relu1(x)
        x = self.conv2(x)
        # torch.Size([50, 8, 28, 28])
        x = self.relu2(x)
        x = self.conv3(x)
        # torch.Size([50, 16, 24, 24])
        x = self.relu3(x)
        x = self.maxpool1(x)
        # torch.Size([50, 16, 12, 12])
        x = self.conv4(x)
        x = self.relu4(x)
        # torch.Size([50, 16, 8, 8])
        x = self.maxpool2(x)
        # torch.Size([50, 16, 4, 4])
        x = x.view(50 ,16*4*4)
        # torch.Size([50, 256])
        x = self.prediction_layer(x)
        return x

class CNN5(torch.nn.Module):
    def __init__(self):
        super(CNN5, self).__init__()
        # Conv. 3x3x8 Layer
        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu1 = torch.nn.ReLU()
        # Conv. 3x3x16 Layer
        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu2 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # ReLU layer
        self.relu3 = torch.nn.ReLU()
        # Conv. 3x3x16 Layer
        self.conv4 = torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu4 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Conv. 3x3x16 Layer
        self.conv5 = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size= (3,3) , stride = 1, padding='valid')
        # ReLU layer
        self.relu5 = torch.nn.ReLU()
        # Conv. 3x3x8 Layer
        self.conv6 = torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride = 1, padding='valid')
        # ReLU layer
        self.relu6 = torch.nn.ReLU()
        # MaxPool 2x2
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size= (2,2), stride = 2, padding=0)
        # Prediction layer provides output with 10 classes
        self.prediction_layer = torch.nn.Linear(in_features= 8*4*4, out_features=10)


    def forward(self, x):

        # torch.Size([50, 1, 32, 32])
        x = self.conv1(x)
        # Size = [50, 8, 30, 30] (guessed)
        x = self.relu1(x)
        x = self.conv2(x)
        # Size = [50, 16, 28, 28] (guessed)
        x = self.relu2(x)
        x = self.conv3(x)
        # Size = [50, 8, 26, 26] (guessed)
        x = self.relu3(x)
        x = self.conv4(x)
        # Size = [50, 16, 24, 24] (guessed)
        x = self.relu4(x)
        x = self.maxpool1(x)
        # Size = [50, 16, 12, 12] (guessed) 
        # MaxPool does NOT efect number of output channels
        x = self.conv5(x)
        # Size = [50, 16, 10, 10] (guessed)
        x = self.relu5(x)
        x = self.conv6(x)
        # Size = [50, 8, 8, 8] (guessed)
        x = self.relu6(x)
        x = self.maxpool2(x)
        # Size = [50, 8, 4, 4] (guessed)
        # torch.Size([50, 8, 4, 4]) (the guess was correct)
        x = x.view(50 ,8*4*4)
        
        x = self.prediction_layer(x)

        
        return x

# Model name (Change the model and model_name)
model_name = 'MLP1'
# Number of tests
num_test = 10

# Batch size is 50
batch_size = 50
# Number of epochs is 15
num_epochs = 15

# Each model is trained 10 times
for x in range(num_test):

  # Create model, loss function, and optimizer
  model = MLP1() 
  model.to(device)
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters())
  
  # Initialize the lists
  train_loss_history = []
  train_acc_history = []
  val_acc_history = []

  for epoch in range(num_epochs):

      for i, (images, labels) in enumerate(train_generator):
          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()
          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)
          images = images.to(device)
          labels = labels.to(device)
          
          # Forward pass and compute loss
          outputs = model(images)
          loss = criterion(outputs, labels)
          
          # Backward and optimize
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
          

          # Record training accuracy every 10 steps
          if (i+1) % 10 == 0:

              # Record training loss
              train_loss_history.append(loss.item())

              with torch.no_grad():
                  total = 0
                  correct = 0

                  #outputs = model(images)
                  _, predicted = torch.max(outputs.data, 1)
                  total += labels.size(0)
                  correct += (predicted == labels).sum().item()
                  train_accuracy = 100 * correct / total
                  train_acc_history.append(train_accuracy)

                  # Record validation accuracy every 10 steps
                  total = 0
                  correct = 0
                  for images, labels in val_generator:
                      images = images.to(device)
                      labels = labels.to(device)
                      outputs = model(images)
                      _, predicted = torch.max(outputs.data, 1)
                      total += labels.size(0)
                      correct += (predicted == labels).sum().item()
                  val_accuracy = 100 * correct / total
                  val_acc_history.append(val_accuracy)

          # Print training loss every 100 steps
          if (i+1) % 100 == 0:
              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                    .format(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))


  print(len(train_loss_history))
  print(len(train_acc_history))
  print(len(val_acc_history))


  # Test the model
  model.eval()
  with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in test_generator:
          # Convert data to PyTorch tensors
          images = images.numpy()
          labels = labels.numpy()
          images = torch.from_numpy(images)
          labels = torch.from_numpy(labels)
          images = images.to(device)
          labels = labels.to(device)
          
          # Compute predictions
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
      test_acc = 100 * correct / total
      print('Accuracy of the model on the test images: {} %'.format(test_acc))

  import pickle

  # create the dictionary object with the required key-value pairs
  result_dict = {
      'name': model_name,
      'loss curve': train_loss_history,
      'train acc curve': train_acc_history,
      'val acc curve': val_acc_history,
      'test acc': test_acc,
      'weights': model.conv1.weight.data
  }

  # save the dictionary object to a file
  filename = 'part3_'+str(model_name)+'_'+str(x+1)+'.pkl'
  with open(filename, 'wb') as file:
      pickle.dump(result_dict, file)

  # load the dictionary object from the file
  with open(filename, 'rb') as file:
      loaded_dict = pickle.load(file)

  # print the loaded dictionary object
  print(loaded_dict)

import pickle

# After traning each model 10 times,
# It is time to take the average of the 
# loss curve, train accuracy, and validation accuracy
# This part is used 5 times for each model

# Model Name
model_name = 'CNN5'

file_name1 = 'part3_'+str(model_name)+'_1.pkl'
file_name2 = 'part3_'+str(model_name)+'_2.pkl'
file_name3 = 'part3_'+str(model_name)+'_3.pkl'
file_name4 = 'part3_'+str(model_name)+'_4.pkl'
file_name5 = 'part3_'+str(model_name)+'_5.pkl'
file_name6 = 'part3_'+str(model_name)+'_6.pkl'
file_name7 = 'part3_'+str(model_name)+'_7.pkl'
file_name8 = 'part3_'+str(model_name)+'_8.pkl'
file_name9 = 'part3_'+str(model_name)+'_9.pkl'
file_name10 = 'part3_'+str(model_name)+'_10.pkl'

# load the dictionary object from the file
with open(file_name1, 'rb') as file:
  loaded_dict1 = pickle.load(file)

# load the dictionary object from the file
with open(file_name2, 'rb') as file:
  loaded_dict2 = pickle.load(file)

# load the dictionary object from the file
with open(file_name3, 'rb') as file:
  loaded_dict3 = pickle.load(file)

# load the dictionary object from the file
with open(file_name4, 'rb') as file:
  loaded_dict4 = pickle.load(file)

# load the dictionary object from the file
with open(file_name5, 'rb') as file:
  loaded_dict5 = pickle.load(file)

# load the dictionary object from the file
with open(file_name6, 'rb') as file:
  loaded_dict6 = pickle.load(file)

# load the dictionary object from the file
with open(file_name7, 'rb') as file:
  loaded_dict7 = pickle.load(file)

# load the dictionary object from the file
with open(file_name8, 'rb') as file:
  loaded_dict8 = pickle.load(file)

# load the dictionary object from the file
with open(file_name9, 'rb') as file:
  loaded_dict9 = pickle.load(file)

# load the dictionary object from the file
with open(file_name10, 'rb') as file:
  loaded_dict10 = pickle.load(file)

dict_name = 'loss curve'
lists = [loaded_dict1[dict_name], loaded_dict2[dict_name], loaded_dict3[dict_name],
         loaded_dict4[dict_name], loaded_dict5[dict_name], loaded_dict6[dict_name],
         loaded_dict7[dict_name], loaded_dict8[dict_name], loaded_dict9[dict_name], loaded_dict10[dict_name]]

# Initialize an empty list to store the averages
loss_curve_average = []

# Use a for loop to iterate over each index in the lists
for i in range(len(loaded_dict1[dict_name])):
    # Initialize a variable to store the sum of the values at that index
    total = 0
    # Use another for loop to iterate over each list in the list of lists
    for lst in lists:
        # Add the value at the current index to the total
        total += lst[i]
    # Calculate the average by dividing the total by the number of lists
    avg = total / len(lists)
    # Append the average to the averages list
    loss_curve_average.append(avg)

dict_name = 'train acc curve'
lists = [loaded_dict1[dict_name], loaded_dict2[dict_name], loaded_dict3[dict_name],
         loaded_dict4[dict_name], loaded_dict5[dict_name], loaded_dict6[dict_name],
         loaded_dict7[dict_name], loaded_dict8[dict_name], loaded_dict9[dict_name], loaded_dict10[dict_name]]

# Initialize an empty list to store the averages
train_acc_curve_average = []

# Use a for loop to iterate over each index in the lists
for i in range(len(loaded_dict1[dict_name])):
    # Initialize a variable to store the sum of the values at that index
    total = 0
    # Use another for loop to iterate over each list in the list of lists
    for lst in lists:
        # Add the value at the current index to the total
        total += lst[i]
    # Calculate the average by dividing the total by the number of lists
    avg = total / len(lists)
    # Append the average to the averages list
    train_acc_curve_average.append(avg)



dict_name = 'val acc curve'
lists = [loaded_dict1[dict_name], loaded_dict2[dict_name], loaded_dict3[dict_name],
         loaded_dict4[dict_name], loaded_dict5[dict_name], loaded_dict6[dict_name],
         loaded_dict7[dict_name], loaded_dict8[dict_name], loaded_dict9[dict_name], loaded_dict10[dict_name]]

# Initialize an empty list to store the averages
val_acc_curve_average = []

# Use a for loop to iterate over each index in the lists
for i in range(len(loaded_dict1[dict_name])):
    # Initialize a variable to store the sum of the values at that index
    total = 0
    # Use another for loop to iterate over each list in the list of lists
    for lst in lists:
        # Add the value at the current index to the total
        total += lst[i]
    # Calculate the average by dividing the total by the number of lists
    avg = total / len(lists)
    # Append the average to the averages list
    val_acc_curve_average.append(avg)

# Finding out which of the runs provides best performance
print(loaded_dict1['test acc'])
print(loaded_dict2['test acc'])
print(loaded_dict3['test acc'])
print(loaded_dict4['test acc'])
print(loaded_dict5['test acc'])
print(loaded_dict6['test acc'])
print(loaded_dict7['test acc'])
print(loaded_dict8['test acc'])
print(loaded_dict9['test acc'])
print(loaded_dict10['test acc'])

# For MLP1, (40.18) second run
# For MLP2, (39.78) first run
# For CNN3, (59.42) tenth run
# For CNN4, (63.42) tenth run
# For CNN5, (58.84) sixth run, gives best test accuracy (performance)

print(loaded_dict6['name'])
best_performance = loaded_dict6['test acc']
weights_of_first_layer = loaded_dict6['weights']


# Creating one final pickle folder for each model

# create the dictionary object with the required key-value pairs
result = {
    'name': model_name,
    'loss_curve': loss_curve_average,
    'train_acc_curve': train_acc_curve_average,
    'val_acc_curve': val_acc_curve_average,
    'test_acc': best_performance,
    'weights': weights_of_first_layer
}

# save the dictionary object to a file
filename = str(model_name)+'.pkl'
with open(filename, 'wb') as file:
    pickle.dump(result, file)

import pickle

# load the dictionary object from the file
with open('MLP1.pkl', 'rb') as file:
  MLP1 = pickle.load(file)
with open('MLP2.pkl', 'rb') as file:
  MLP2 = pickle.load(file)
with open('CNN3.pkl', 'rb') as file:
  CNN3 = pickle.load(file)
with open('CNN4.pkl', 'rb') as file:
  CNN4 = pickle.load(file)
with open('CNN5.pkl', 'rb') as file:
  CNN5 = pickle.load(file)

dict_models = [MLP1,MLP2,CNN3,CNN4,CNN5]

print('### Test Accuracies ###')
print('MLP1: ',MLP1['test_acc'])
print('MLP2: ',MLP2['test_acc'])
print('CNN3: ',CNN3['test_acc'])
print('CNN4: ',CNN4['test_acc'])
print('CNN5: ',CNN5['test_acc'])

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
import os
import torch
from torchvision.utils import make_grid

# utility function to create performance plots for part 3
def part3Plots(results, save_dir='', filename='', show_plot=True):

    color_list = ['#0000ff', '#ff0000', '#d2691e', '#ff00ff', '#00ff00', '#000000', '#373788']
    style_list = ['-', '--']

    num_results = len(results)

    plot_curve_args = [{'c': color_list[k],
                        'linestyle': style_list[0],
                        'linewidth': 2} for k in range(num_results)]

    plot_point_args = [{'c': color_list[k],
                        'marker': 'o',
                        'markersize': 9,
                        'markerfacecolor':  color_list[k]} for k in range(num_results)]



    font_size = 18

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))


    # training loss
    ax = axes[0, 0]
    ax.set_title('training_loss', loc='left', fontsize=font_size)
    for result, args in zip(results, plot_curve_args):
        ax.plot(np.arange(1, len(result['loss_curve']) + 1), result['loss_curve'], label=result['name'], **args)
        ax.set_xlabel(xlabel='step', fontsize=font_size)
        ax.set_ylabel(ylabel='loss', fontsize=font_size)
        ax.tick_params(labelsize=12)

    # get lines for global legend
    lines = ax.get_lines()


    # training and validation accuracy
    ax = axes[0, 1]
    ax.set_title('train_and_val_accuracies', loc='right', fontsize=font_size)
    for result, args in zip(results, plot_curve_args):
        ax.plot(np.arange(1, len(result['train_acc_curve']) + 1), result['train_acc_curve'], label=result['name'],
                **args)
        args['linestyle'] = style_list[1]
        ax.plot(np.arange(1, len(result['val_acc_curve']) + 1), result['val_acc_curve'], label=result['name'],
                **args)
        args['linestyle'] = style_list[0]
        ax.set_xlabel(xlabel='step', fontsize=font_size)
        ax.set_ylabel(ylabel='acc.', fontsize=font_size)
        ax.tick_params(labelsize=12)

        legend_elements = [Line2D([0], [0], color='k', linestyle=style_list[0], lw=2, label='train.'),
                           Line2D([0], [0], color='k', linestyle=style_list[1], lw=2, label='val.')]

        ax.legend(fontsize=12, loc='best', handles=legend_elements)

    # validation vs training accuracy
    ax = axes[1, 1]
    ax.set_title('validation_vs_training_accuracy', loc='right', fontsize=font_size)
    for result, args in zip(results, plot_curve_args):
        ax.plot(result['train_acc_curve'], result['val_acc_curve'], label=result['name'], **args)
        ax.set_xlabel(xlabel='training', fontsize=font_size)
        ax.set_ylabel(ylabel='validation', fontsize=font_size)
        ax.tick_params(labelsize=12)



    # test vs training accuracy
    ax = axes[1, 0]
    ax.set_title('test_vs_training_accuracy', loc='left', fontsize=font_size)
    for result, args in zip(results, plot_point_args):
        train_acc = result['train_acc_curve'][-1]
        test_acc = result['test_acc']
        ax.plot(train_acc, test_acc, label=result['name'],  **args)
        ax.set_xlabel(xlabel='training', fontsize=font_size)
        ax.set_ylabel(ylabel='test', fontsize=font_size)
        ax.tick_params(labelsize=12)

    # global legend
    fig.legend(labels=[line._label for line in lines],
               ncol=3, loc="upper center", fontsize=font_size,
               handles=lines)

    if show_plot:
        plt.show()

    fig.savefig(os.path.join(save_dir, filename + '.png'))


# utility function to visualize learned weights
def visualizeWeights(weights, save_dir, filename='weigths'):
    '''visualizes the weights and saves the visualization as a png image

    Arguments:
    ----------
    weights : numpy array of size 1024 x D where D is the number of weights
    save_dir : string, path to directory to save the image
    filename : strint, name of the saved image (.png is to be appended automatically)

    Example:
    --------
    visualizing weights at the input layer of a keras.Model object

    # assume classifier is an instance of keras.Model

    >>> weights = classifier.trainable_weights[0].numpy()

    >>> visualizeWeights(weights, save_dir='some\location\to\save', filename='input_weights')
    '''

    weights = weights.T

    num_weights = weights.shape[-1]

    dim = np.ceil(np.sqrt(num_weights)).astype(int)

    fig, axes = plt.subplots(dim, dim)

    # use global min / max to ensure all weights are shown on the same scale
    vmin, vmax = weights.min(), weights.max()
    for coef, ax in zip(weights.T, axes[:num_weights].ravel()):
        coef = np.squeeze(coef).T if len(weights.shape) > 2 else coef.reshape(32, 32)
        ax.matshow(coef, cmap=plt.cm.gray, vmin=.5 * vmin,
                   vmax=.5 * vmax)

        ax.set_xticks(())
        ax.set_yticks(())

    fig.show()

    fig.savefig(os.path.join(save_dir, filename + '.png'))

# utility function to visualize dataset
def visualizeDataset(images, labels, save_dir, filename='dataset', num_samples_per_class=8):

    num_classes = np.max(labels) + 1

    fig, axes = plt.subplots(num_classes, num_samples_per_class)

    images = (images - np.min(images)) / (np.max(images) - np.min(images))

    for r in range(num_classes):
        sample_indcs = np.where(labels == r)[0][:num_samples_per_class]
        for n in range(num_samples_per_class):
            axes[r, n].matshow(images[sample_indcs[n]].reshape(32, 32),
                               cmap=plt.cm.gray)
            axes[r, n].set_xticks(())
            axes[r, n].set_yticks(())

    fig.show()

    fig.savefig(os.path.join(save_dir, filename + '.png'))

part3Plots(dict_models, save_dir=r'data', filename='part3Plots')

weight = MLP2['weights'].cpu().numpy()
visualizeWeights(weight, save_dir='data', filename='weight_plot_MLP2')

### End of Question 3 ###